{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde1063a",
   "metadata": {},
   "source": [
    "# Agreement analysis between reviewers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecfa9db",
   "metadata": {},
   "source": [
    "The objective of this analysis is to look for agreement/disagreement between reviewers' interpretation in systematic reviews. The sustained hypothesis on this matter is that reviewers will not agree on text interpretation and technical details of papers.\n",
    "\n",
    "The data consists of 3 groups of 23 observations of 11 variables: title of manuscripts, url, full abstract, publication date, review (boolean), llm (boolean), set of llms used, structured_data (boolean), list of medical conditions, and evaluate_patient_trial. Each group represents a reviewer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71723bb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74babc9d",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbda76b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd101113",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "\n",
    "Here we are loading the reviewer files. Please note that the answers have been manually reviewed and small changes/ fix-ups have been done where needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "234991ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviewer1 = pd.read_excel('./results/FullText_reviewer_1.xlsx')\n",
    "GPT = pd.read_excel('./results/FullText_reviewer_GPT4.xlsx')\n",
    "Resolution = pd.read_excel('./results/FullText_resolution.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9afbb4",
   "metadata": {},
   "source": [
    "## Calculating Inter-rater Agreement\n",
    "\n",
    "We have four functions to calculate inter-rater agreement. `kappa_calculation` is the main fuction that calculates Cohen's Kappa for two lists containing 'yes'/'no' values. `kappa_boolean` and `kappa_non_boolean` re-format the answers from the reviewers into 'yes'/'no' lists and call on `kappa_calculation` to generate the agreement values. Finally, `Kappa` is the function that puts everything together calling either `kappa_boolean` or `kappa_non_boolean` for each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb5aa0",
   "metadata": {},
   "source": [
    "### Kappa Calculation\n",
    "\n",
    "The code for this Kappa Calculation was taken from this page: https://rowannicholls.github.io/python/statistics/agreement/cohens_kappa.html\n",
    "\n",
    "The formula for the standard deviation is found in equation 7 of Cohen (1960) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab4af9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa_calculation(List1, List2):\n",
    "    \"\"\" Function that calculates Cohen's Kappa coefficient for two lists that contain 'yes' or 'no' answers.\n",
    "    Please note that the input lists should have the same length.\n",
    "\n",
    "    Parameters:\n",
    "    List1, List2 (list['yes'|'no']): lists of 'yes' or 'no' values.\n",
    "\n",
    "    Returns:\n",
    "    kappa, (lower, upper)\n",
    "    A float represesenting the calculated Cohen's Kappa coefficient of the two lists and a tuple of two floats\n",
    "    representing the approximate lower and the upper ends of the confidence interval for the kappa coefficient.\n",
    "    \"\"\"\n",
    "\n",
    "    readerA = List1\n",
    "    readerB = List2\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(readerA, readerB, labels=['yes','no'])\n",
    "\n",
    "    # Sample size\n",
    "    n = np.sum(cm)\n",
    "\n",
    "    # Expected matrix\n",
    "    sum0 = np.sum(cm, axis=0)\n",
    "    sum1 = np.sum(cm, axis=1)\n",
    "    expected = np.outer(sum0, sum1) / n**2\n",
    "\n",
    "    # Number of classes\n",
    "    n_classes = cm.shape[0]\n",
    "\n",
    "    # Calculate p_o (the observed proportion of agreement) and\n",
    "    # p_e (the probability of random agreement)\n",
    "    identity = np.identity(n_classes)\n",
    "    p_o = np.sum((identity * cm) / n)\n",
    "    p_e = np.sum((identity * expected))\n",
    "    # Calculate Cohen's kappa\n",
    "    kappa = (p_o - p_e) / (1 - p_e)\n",
    "\n",
    "    # Approximate confidence intervals\n",
    "    # Equation 7 of Cohen (1960)\n",
    "    se = np.sqrt((p_o * (1 - p_o)) / (n * (1 - p_e)**2))\n",
    "    ci = 1.96 * se * 2\n",
    "    lower = kappa - 1.96 * se\n",
    "    upper = kappa + 1.96 * se\n",
    "\n",
    "    #display only upto two decimal places\n",
    "\n",
    "    kappa = float(f\"{kappa:.2f}\")\n",
    "    lower = float(f\"{lower:.2f}\")\n",
    "    upper = float(f\"{upper:.2f}\")\n",
    "\n",
    "    return kappa, (lower, upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1943351",
   "metadata": {},
   "source": [
    "### Boolean columns\n",
    "\n",
    "The following function is used to process values for columns that have boolean values (**llm**, **review** and **structured_data** )\n",
    "\n",
    "Its main purpose is to convert all the YES/NO values in the list to their lowercase form because `kappa_calculation` needs consistent 'yes' or 'no' values.\n",
    "\n",
    "**NOTE: Please make sure the all the values from reviewers are either 'YES'/'yes' or 'NO'/'no', otherwise it may throw an error.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95b77258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa_boolean(List1, List2):\n",
    "    \"\"\" Function calculating Cohen's Kappa coefficient for two lists of boolean values. Reviewers were asked\n",
    "    to answer \"YES\" or \"NO\", so those answers need to be converted to lowercase.\n",
    "\n",
    "    Parameters:\n",
    "    List1, List2 (list[str]): Answers retrieved by the reviewers, should be \"YES\" or \"NO\"\n",
    "\n",
    "    Returns:\n",
    "    kappa, (lower, upper)\n",
    "    A float represesenting the calculated Cohen's Kappa coefficient of the two lists and a tuple of\n",
    "    two floats representing the lower and the upper ends of the confidence interval for the kappa coefficient.\n",
    "    \"\"\"\n",
    "\n",
    "    List1 = [(val).lower() for val in List1]\n",
    "    List2 = [(val).lower() for val in List2]\n",
    "\n",
    "    return kappa_calculation(List1, List2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8bd077",
   "metadata": {},
   "source": [
    "### Non Boolean Columns\n",
    "\n",
    "\n",
    "This function will be used to process non boolean columns like **llm_name** and **list_of_medical_conditions**. The values in these columns represent lists of tokens separated by commas. We used a one-hot encoding with the union of all values for each entry to generate a list of 'yes'/'no' answers and calculate agreement with `kappa_calculation`. Here is an example of how the function works:\n",
    "\n",
    "Say the responses to the first article are\n",
    "\n",
    "    List1[0] = ['BERT', 'ClinicalBERT']\n",
    "    List2[0] = ['BIOBERT', 'BERT'].\n",
    "\n",
    "To create the one-hot encodings corresponding to the first article, we first make a union vector of all responses\n",
    "\n",
    "    UNION = ['BERT','ClinicalBERT','BIOBERT']\n",
    "\n",
    "Next, the encoding for each list will be a vector of same length as UNION with the i-th entry being 'yes' if UNION[i] is in the list and 'no' otherwise. For our example we'll have\n",
    "\n",
    "    list1: ['yes', 'yes', 'no']\n",
    "    list2: ['yes', 'no', 'yes']\n",
    "\n",
    "This process is repeated for all articles and the one-hot-encodings are concatenated.\n",
    "\n",
    "We treat the case in which neither of the reviewers identified any tokens as both reviewers agreeing that the answer should be null, so we add a 'yes' to each of the one-hot encoding lists.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "343d0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace_and_capitalize(input_string):\n",
    "    \"\"\" Helper function used to pre-process the text in a list of token data. This function is used to ensure\n",
    "    that casing and white space are ignored when comparing answers from reviewers. For xample, 'gpt3' will\n",
    "    be equivalent to GPT3' and 'Clinical BERT' to 'clinicalBERT'\n",
    "    \"\"\"\n",
    "    # Remove white spaces\n",
    "    no_whitespace = input_string.replace(\" \", \"\")\n",
    "\n",
    "    # Convert to uppercase\n",
    "    uppercase_string = no_whitespace.upper()\n",
    "\n",
    "    return uppercase_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7329890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa_non_boolean(List1, List2):\n",
    "    \"\"\" Function that converts list-of-tokens responses for non boolean parameters into boolean vectors\n",
    "    and calculates the inter-rater agreement.\n",
    "\n",
    "    Parameters:\n",
    "    List1, List2 (list[str]): lists representing the extracted values for non-boolean parameters. Please note that for non-boolean data\n",
    "                              each entry may have several answers. For example, several LLMs might have been used in a single article.\n",
    "                              In this case, the entry corresponding to that article would be a string enumeration of all the LLMs used.\n",
    "                              ex: ['', 'BERT, ClinicalBERT' , 'Glove, BERT', '', ...]\n",
    "\n",
    "    Returns:\n",
    "    kappa, (lower, upper)\n",
    "    A float represesenting the calculated Cohen's Kappa coefficient of the two lists and a tuple of\n",
    "    two floats representing the lower and the upper ends of the confidence interval for the kappa coefficient.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    one_hot_list1 = []\n",
    "    one_hot_list2 = []\n",
    "\n",
    "    for ind in range(len(List1)):\n",
    "\n",
    "        # remove white space and make every string to UpperCase\n",
    "        string1 = remove_whitespace_and_capitalize(str(List1[ind])).split(',')\n",
    "        string2 = remove_whitespace_and_capitalize(str(List2[ind])).split(',')\n",
    "\n",
    "        # remove empty string or nan values\n",
    "        string1 = [item for item in string1 if item != '' and item != 'NAN']\n",
    "        string2 = [item for item in string2 if item != '' and item != 'NAN']\n",
    "\n",
    "        # Make a union list for each index.\n",
    "        UNION = list(set(string1) | set(string2))\n",
    "\n",
    "        # if neither of the reviewers identified any values for the parameter, then they agree that the answer should be null,\n",
    "        # so we will add a 'yes' to each of the one-hot-encoding lists.\n",
    "        if len(UNION) == 0:\n",
    "            one_hot_list1.append('yes')\n",
    "            one_hot_list2.append('yes')\n",
    "\n",
    "        # otherwise, we generate one-hot encodings of 'yes' and 'no based on presence of entries in the UNION\n",
    "        else:\n",
    "            for each_item in UNION:\n",
    "\n",
    "                if each_item in string1:\n",
    "                    one_hot_list1.append('yes')\n",
    "                else:\n",
    "                    one_hot_list1.append('no')\n",
    "\n",
    "                if each_item in string2:\n",
    "                    one_hot_list2.append('yes')\n",
    "                else:\n",
    "                    one_hot_list2.append('no')\n",
    "\n",
    "    # kappa_calculation\n",
    "    final_ans = kappa_calculation(one_hot_list1, one_hot_list2)\n",
    "\n",
    "    return final_ans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f24a19a",
   "metadata": {},
   "source": [
    "### Agreement between a pair of reviewers\n",
    "\n",
    "This function calculates the inter-rater agreement between two reviewers for all the parameters of interest. We still use Cohen's kappa for the main agreement value, as we did in the Screening phase, but adjust the variance estimates following the jackknife approach from Blackman and Koval (2000).\n",
    "\n",
    "The approximate variance given in Cohen (1960, equation 7) used previously is known to suffer from asymmetries in the data. The jackknife approach, seen in Blackman and Koval (2000) provides a less inflated variance estimate. The confidence interval is still based on an asymptotic normal distribution of the estimated coefficient of agreement and can still lead to values outside of the range. However, these will almost certainly be closer to the true range of the possible values of kappa.\n",
    "\n",
    "Jackknife estimation of the variance consists of getting a first estimate of kappa and then, one at a time, removing an observation and calculating the new estimate. These estimates obtained by \"leaving one out\" are used in the jackknife variance formula\n",
    "\n",
    "$$estimated\\_variance = \\frac{n-1}{n} * sum\\_of\\_squared\\_differences(all\\_leave\\_one\\_out\\_kappas, kappa\\_using\\_all\\_data)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3effa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kappa(rater1, rater2):\n",
    "    \"\"\" Function that calculates inter-rater agreement between `rater1` and `rater2` across all parameters\n",
    "    of interest.\n",
    "\n",
    "    Parameters:\n",
    "    rater1, rater2: dataframes corresponding to the two reviewers. It is assumed that the dataframes have a column\n",
    "                    for each parameter of interest (defined within the body of the function), that the numer of rows\n",
    "                    is equal and the answers follow expected formatting (YES/NO, list of tokens, etc.)\n",
    "\n",
    "    Returns:\n",
    "    dict {str: (float, (float, float))}\n",
    "    A collection mapping each parameter to the calculated Cohen's Kappa and confidence interval.\n",
    "    \"\"\"\n",
    "\n",
    "    n = rater1.shape[0]\n",
    "    boolean_columns = ['review','llm', 'structured_data']\n",
    "    non_boolean_columns = ['llm_name', 'list_of_medical_conditions']\n",
    "    result = {}\n",
    "\n",
    "    for column in boolean_columns:\n",
    "        List1 = rater1[column].to_list()\n",
    "        List2 = rater2[column].to_list()\n",
    "\n",
    "        result[column] = {}\n",
    "        pseudoKappas = []\n",
    "        kappaEstimate, _ = kappa_boolean(List1,List2)\n",
    "\n",
    "        for idx_to_remove in range(n):\n",
    "            # leave out the `idx_to_remove` observation\n",
    "            pseudoRater1 = rater1.drop(idx_to_remove).reset_index(drop=True)\n",
    "            pseudoRater2 = rater2.drop(idx_to_remove).reset_index(drop=True)\n",
    "\n",
    "            pseudoList1 = pseudoRater1[column].to_list()\n",
    "            pseudoList2 = pseudoRater2[column].to_list()\n",
    "\n",
    "            # calculate the kappa coefficient with all observations except `idx_to_remove`\n",
    "            pseudoKappa, _ = kappa_boolean(pseudoList1,pseudoList2)\n",
    "            if not np.isnan(pseudoKappa):\n",
    "                pseudoKappas.append(pseudoKappa)\n",
    "\n",
    "        pseudoKappas = np.array(pseudoKappas)\n",
    "\n",
    "        # calculate jackknife estimation for variance, and CI bounds\n",
    "        estimatedVariance = ((n-1)/n)*np.sum((pseudoKappas-kappaEstimate)**2)\n",
    "        lower = np.around(kappaEstimate-1.96*np.sqrt(estimatedVariance), 2)\n",
    "        upper = np.around(kappaEstimate+1.96*np.sqrt(estimatedVariance), 2)\n",
    "        result[column] = (kappaEstimate, (lower, upper))\n",
    "\n",
    "    for column in non_boolean_columns:\n",
    "        List1 = rater1[column].to_list()\n",
    "        List2 = rater2[column].to_list()\n",
    "\n",
    "        result[column] = {}\n",
    "        pseudoKappas = []\n",
    "        kappaEstimate, _ = kappa_non_boolean(List1,List2)\n",
    "\n",
    "        for idx_to_remove in range(n):\n",
    "            # leave out the `idx_to_remove` observation\n",
    "            pseudoRater1 = rater1.drop(idx_to_remove).reset_index(drop=True)\n",
    "            pseudoRater2 = rater2.drop(idx_to_remove).reset_index(drop=True)\n",
    "\n",
    "            pseudoList1 = pseudoRater1[column].to_list()\n",
    "            pseudoList2 = pseudoRater2[column].to_list()\n",
    "\n",
    "            # calculate the kappa coefficient with all observations except `idx_to_remove`\n",
    "            pseudoKappa, _ = kappa_non_boolean(pseudoList1,pseudoList2)\n",
    "            if not np.isnan(pseudoKappa):\n",
    "                pseudoKappas.append(pseudoKappa)\n",
    "\n",
    "        pseudoKappas = np.array(pseudoKappas)\n",
    "\n",
    "        # calculate jackknife estimation for variance, and CI bounds\n",
    "        estimatedVariance = ((n-1)/n)*np.sum((pseudoKappas-kappaEstimate)**2)\n",
    "        lower = np.around(kappaEstimate-1.96*np.sqrt(estimatedVariance), 2)\n",
    "        upper = np.around(kappaEstimate+1.96*np.sqrt(estimatedVariance), 2)\n",
    "        result[column] = (kappaEstimate, (lower, upper))\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8cec4",
   "metadata": {},
   "source": [
    "## Running the code\n",
    "\n",
    "Below we calculate and display the agreement values for each pair of reviewers and consensus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07a597c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4343/3089303939.py:37: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  kappa = (p_o - p_e) / (1 - p_e)\n",
      "/tmp/ipykernel_4343/3089303939.py:41: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  se = np.sqrt((p_o * (1 - p_o)) / (n * (1 - p_e)**2))\n",
      "/tmp/ipykernel_4343/3089303939.py:37: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  kappa = (p_o - p_e) / (1 - p_e)\n",
      "/tmp/ipykernel_4343/3089303939.py:41: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  se = np.sqrt((p_o * (1 - p_o)) / (n * (1 - p_e)**2))\n",
      "/tmp/ipykernel_4343/3089303939.py:37: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  kappa = (p_o - p_e) / (1 - p_e)\n",
      "/tmp/ipykernel_4343/3089303939.py:41: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  se = np.sqrt((p_o * (1 - p_o)) / (n * (1 - p_e)**2))\n",
      "/tmp/ipykernel_4343/3089303939.py:37: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  kappa = (p_o - p_e) / (1 - p_e)\n",
      "/tmp/ipykernel_4343/3089303939.py:41: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  se = np.sqrt((p_o * (1 - p_o)) / (n * (1 - p_e)**2))\n"
     ]
    }
   ],
   "source": [
    "gpt_reviewer1 = Kappa(GPT,Reviewer1)\n",
    "gpt_resolution = Kappa(GPT,Resolution)\n",
    "reviewer1_resolution = Kappa(Reviewer1,Resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d936b072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': (0.0, (0.0, 0.0)),\n",
       " 'llm': (0.47, (0.04784264215507422, 0.8921573578449258)),\n",
       " 'structured_data': (0.55, (0.16757609679946733, 0.9324239032005328)),\n",
       " 'llm_name': (-0.35, (-0.7039815961809068, 0.003981596180906877)),\n",
       " 'list_of_medical_conditions': (-0.2,\n",
       "  (-0.34848381554317925, -0.05151618445682077))}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_reviewer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05268fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': (0.0, (0.0, 0.0)),\n",
       " 'llm': (0.59, (0.2090201192120278, 0.9709798807879721)),\n",
       " 'structured_data': (0.64, (0.29548768180033924, 0.9845123181996608)),\n",
       " 'llm_name': (-0.3, (-0.5669957563577434, -0.033004243642256625)),\n",
       " 'list_of_medical_conditions': (-0.25,\n",
       "  (-0.40093825976494346, -0.09906174023505651))}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4778b56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': (1.0, (1.0, 1.0)),\n",
       " 'llm': (0.89, (0.6739743332479862, 1.1060256667520139)),\n",
       " 'structured_data': (0.91, (0.73747739656598, 1.08252260343402)),\n",
       " 'llm_name': (0.0, (0.0, 0.0)),\n",
       " 'list_of_medical_conditions': (-0.16,\n",
       "  (-0.33982283068282126, 0.019822830682821257))}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewer1_resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d992bd3",
   "metadata": {},
   "source": [
    "## Justification  \n",
    "\n",
    "Literature review stands on two pillars: the guidelines for inclusion of documents and the subjejctive view of the reviewer. The guideles presented in a PRISMA review are well accepted and consolidated. They are widely used and accept as a generally good choice for the first pillar. Subjectivity of the reviewer after that point is under considered in the discussions of the quality of systematic reviews. There is ambiguity of varying degrees in scientific texts and it may influence systematic reviews. \n",
    "\n",
    "One approach on dealing with subjectiviness of reviews, is to inspect agreement between reviewers. In this analysis a method is proposed for this. Consider a corpora of texts included in a review and consider the representation of the entities, in the sense of natural language processing, as tuples ($entity, semantics, attributes$). When analysing the corpora, all the entities of all the texts are analysed after discovery. For more details on this, see Bird et al. (2009), for example. The proposal is to consider all entities presented in the texts and evaluate, at the very least, if the same entities are considered as belonging to the same texts by independent reviewers. If pairs of reviewers cannot agree about a particular entity belonging to a text or not, that would be a clear indication of ambiguity in texts. The method is applied to pair of reviewers and consists in traversing the corpora of papers. For each paper the the union of the sets of entities annotated by each reviewer is obtained. A data frame is created with each row consisting of an entity in the union set and each collumn represents a list of boolean values indicating if each reviewer included or not the entity in that manuscript. The data frames of all papers are then stacked and Cohen's $\\kappa$ coefficient of agreement is calculated for the lists of boolean values. A higher value of $\\kappa$ indicates consensus between reviewers, the opposite being true for lower values. It is important to take notice that this proposal is aligned to the assumptions of Cohen (1960).\n",
    "\n",
    "This idea is similar to was used previously in Liu et al. (2018), where Cohen's kappa is used as an ad-hoc metric for agreement of machine learning method. McHugh (2012) presents a suggestion of use of Cohen's kappa that creates a list of yes/no observations based on perceived scores by professionals in a way that is very similar to the proposed one-hot encoding presented here.  \n",
    "\n",
    "One particular issue with data on systematic reviews is that sample size can be very restrictive. Most of the widely used formulas for $\\kappa$ confidence intervals are approximations based on asymptotic properties of the sample distributions of the proportions involved in its calculation. To avoid conveergence issues or nonsene values for the confidence intervals, one alternative is to use the Jackknife estimator of the variance of the $\\kappa$ under the null hypothesis described in Cohen (1960). This is seen, for instance, in Blackman and Koval (2000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f120d147",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Blackman, N. J., & Koval, J. J. (2000). Interval estimation for Cohen’s kappa as a measure of agreement. Statistics in Medicine, 19(5), 723–741. https://doi.org/10.1002/(sici)1097-0258(20000315)19:5<723::aid-sim379>3.0.co;2-a\n",
    "\n",
    "2. Bird, S, klein, E., & Loper, E. (2009). Natural Language Processing with Python. O'Reilly, Canada.\n",
    "\n",
    "3. Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20, 37–46. https://doi.org/10.1177/001316446002000104\n",
    "\n",
    "4. Liu, W., Luo, Z., & Li, S. (2018). Improving deep ensemble vehicle classification by using selected adversarial samples. Knowledge-Based Systems, 160, 167–175. https://doi.org/10.1016/j.knosys.2018.06.035\n",
    "\n",
    "5. McHugh, M. L. (2012). Interrater reliability: The kappa statistic. Biochemia Medica, 22(3), 276–282. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
