{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74babc9d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbda76b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\otili\\Desktop\\Upwork\\PRISMA project\\Kappa_new.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/otili/Desktop/Upwork/PRISMA%20project/Kappa_new.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/otili/Desktop/Upwork/PRISMA%20project/Kappa_new.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/otili/Desktop/Upwork/PRISMA%20project/Kappa_new.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd101113",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "\n",
    "Here we are loading the reviewer files. Please note that the answers have been manually reviewed and small changes/ fix-ups have been done where needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234991ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviewer1 = pd.read_excel('./results/FullText_reviewer_1.xlsx')\n",
    "GPT = pd.read_excel('./results/FullText_reviewer_GPT4.xlsx')\n",
    "Resolution = pd.read_excel('./results/FullText_resolution.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9afbb4",
   "metadata": {},
   "source": [
    "# Calculating Inter-rater Agreement\n",
    "\n",
    "We have four functions to calculate inter-rater agreement. `kappa_calculation` is the main fuction that calculates Cohen's Kappa for two lists containing 'yes'/'no' values. `kappa_boolean` and `kappa_non_boolean` re-format the answers from the reviewers into 'yes'/'no' lists and call on `kappa_calculation` to generate the agreement values. Finally, `Kappa` is the function that puts everything together calling either `kappa_boolean` or `kappa_non_boolean` for each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb5aa0",
   "metadata": {},
   "source": [
    "## Kappa Calculation\n",
    "\n",
    "The code for this Kappa Calculation was taken from this page: https://rowannicholls.github.io/python/statistics/agreement/cohens_kappa.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4af9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa_calculation(List1, List2):\n",
    "    \"\"\" Function that calculates Cohen's Kappa coefficient for two lists that contain 'yes' or 'no' answers.\n",
    "    Please note that the input lists should have the same length.\n",
    "\n",
    "    Parameters:\n",
    "    List1, List2 (list['yes'|'no']): lists of 'yes' or 'no' values.\n",
    "\n",
    "    Returns:\n",
    "    kappa, (lower, upper)\n",
    "    A float represesenting the calculated Cohen's Kappa coefficient of the two lists and a tuple of two floats\n",
    "    representing the lower and the upper ends of the confidence interval for the kappa coefficient.\n",
    "    \"\"\"\n",
    "\n",
    "    readerA = List1\n",
    "    readerB = List2\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(readerA, readerB, labels=['yes','no'])\n",
    "\n",
    "    # Sample size\n",
    "    n = np.sum(cm)\n",
    "\n",
    "    # Expected matrix\n",
    "    sum0 = np.sum(cm, axis=0)\n",
    "    sum1 = np.sum(cm, axis=1)\n",
    "    expected = np.outer(sum0, sum1) / n\n",
    "\n",
    "    # Number of classes\n",
    "    n_classes = cm.shape[0]\n",
    "\n",
    "    # Calculate p_o (the observed proportionate agreement) and\n",
    "    # p_e (the probability of random agreement)\n",
    "    identity = np.identity(n_classes)\n",
    "    p_o = np.sum((identity * cm) / n)\n",
    "    p_e = np.sum((identity * expected) / n)\n",
    "\n",
    "    # Calculate Cohen's kappa\n",
    "    kappa = (p_o - p_e) / (1 - p_e)\n",
    "\n",
    "    # Confidence intervals\n",
    "    se = np.sqrt((p_o * (1 - p_o)) / (n * (1 - p_e)**2))\n",
    "    ci = 1.96 * se * 2\n",
    "    lower = kappa - 1.96 * se\n",
    "    upper = kappa + 1.96 * se\n",
    "\n",
    "    #display only upto two decimal places\n",
    "\n",
    "    kappa = float(f\"{kappa:.2f}\")\n",
    "    lower = float(f\"{lower:.2f}\")\n",
    "    upper = float(f\"{upper:.2f}\")\n",
    "\n",
    "    return kappa, (lower, upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1943351",
   "metadata": {},
   "source": [
    "## Boolean columns\n",
    "\n",
    "The following function is used to process values for columns that have boolean values (**llm**, **review** and **structured_data** )\n",
    "\n",
    "Its main purpose is to convert all the YES/NO values in the list to their lowercase form because `kappa_calculation` needs consistent 'yes' or 'no' values.\n",
    "\n",
    "**NOTE: Please make sure the all the values from reviewers are either 'YES'/'yes' or 'NO'/'no', otherwise it may throw an error.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b77258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa_boolean(List1, List2):\n",
    "    \"\"\" Function calculating Cohen's Kappa coefficient for two lists of boolean values. Reviewers were asked\n",
    "    to answer \"YES\" or \"NO\", so those answers need to be converted to lowercase.\n",
    "\n",
    "    Parameters:\n",
    "    List1, List2 (list[str]): Answers retrieved by the reviewers, should be \"YES\" or \"NO\"\n",
    "\n",
    "    Returns:\n",
    "    kappa, (lower, upper)\n",
    "    A float represesenting the calculated Cohen's Kappa coefficient of the two lists and a tuple of\n",
    "    two floats representing the lower and the upper ends of the confidence interval for the kappa coefficient.\n",
    "    \"\"\"\n",
    "\n",
    "    List1 = [(val).lower() for val in List1]\n",
    "    List2 = [(val).lower() for val in List2]\n",
    "\n",
    "    return kappa_calculation(List1, List2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8bd077",
   "metadata": {},
   "source": [
    "## Non Boolean Columns\n",
    "\n",
    "\n",
    "This function will be used to process non boolean columns like **llm_name** and **list_of_medical_conditions**. The values in these columns represent lists of tokens separated by commas. We used a one-hot encoding with the union of all values for each entry to generate a list of 'yes'/'no' answers and calculate agreement with `kappa_calculation`. Here is an example of how the function works:\n",
    "\n",
    "Say the responses to the first article are\n",
    "\n",
    "    List1[0] = ['BERT', 'ClinicalBERT']\n",
    "    List2[0] = ['BIOBERT', 'BERT'].\n",
    "\n",
    "To create the one-hot encodings corresponding to the first article, we first make a union vector of all responses\n",
    "\n",
    "    UNION = ['BERT','ClinicalBERT','BIOBERT']\n",
    "\n",
    "Next, the encoding for each list will be a vector of same length as UNION with the i-th entry being 'yes' if UNION[i] is in the list and 'no' otherwise. For our example we'll have\n",
    "\n",
    "    list1: ['yes', 'yes', 'no']\n",
    "    list2: ['yes', 'no', 'yes']\n",
    "\n",
    "This process is repeated for all articles and the one-hot-encodings are concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace_and_capitalize(input_string):\n",
    "    \"\"\" Helper function used to pre-process the text in a list of token data. This function is used to ensure\n",
    "    that casing and white space are ignored when comparing answers from reviewers. For xample, 'gpt3' will\n",
    "    be equivalent to GPT3' and 'Clinical BERT' to 'clinicalBERT'\n",
    "    \"\"\"\n",
    "    # Remove white spaces\n",
    "    no_whitespace = input_string.replace(\" \", \"\")\n",
    "\n",
    "    # Convert to uppercase\n",
    "    uppercase_string = no_whitespace.upper()\n",
    "\n",
    "    return uppercase_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7329890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa_non_boolean(List1, List2):\n",
    "    \"\"\" Function that converts list-of-tokens responses for non boolean parameters into boolean vectors\n",
    "    and calculates the inter-rater agreement.\n",
    "\n",
    "    Parameters:\n",
    "    List1, List2 (list[str]): lists representing the extracted values for non-boolean parameters. Please note that for non-boolean data\n",
    "                              each entry may have several answers. For example, several LLMs might have been used in a single article.\n",
    "                              In this case, the entry corresponding to that article would be a string enumeration of all the LLMs used.\n",
    "                              ex: ['', 'BERT, ClinicalBERT' , 'Glove, BERT', '', ...]\n",
    "\n",
    "    Returns:\n",
    "    kappa, (lower, upper)\n",
    "    A float represesenting the calculated Cohen's Kappa coefficient of the two lists and a tuple of\n",
    "    two floats representing the lower and the upper ends of the confidence interval for the kappa coefficient.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    one_hot_list1 = []\n",
    "    one_hot_list2 = []\n",
    "\n",
    "    for ind in range(len(List1)):\n",
    "\n",
    "        # remove white space and make every string to UpperCase\n",
    "        string1 = remove_whitespace_and_capitalize(str(List1[ind])).split(',')\n",
    "        string2 = remove_whitespace_and_capitalize(str(List2[ind])).split(',')\n",
    "\n",
    "        # remove empty string or nan values\n",
    "        string1 = [item for item in string1 if item != '' and item != 'NAN']\n",
    "        string2 = [item for item in string2 if item != '' and item != 'NAN']\n",
    "\n",
    "        # Make a union list for each index.\n",
    "        UNION = list(set(string1) | set(string2))\n",
    "\n",
    "        for each_item in UNION:\n",
    "\n",
    "            if each_item in string1:\n",
    "                one_hot_list1.append('yes')\n",
    "            else:\n",
    "                one_hot_list1.append('no')\n",
    "\n",
    "            if each_item in string2:\n",
    "                one_hot_list2.append('yes')\n",
    "            else:\n",
    "                one_hot_list2.append('no')\n",
    "\n",
    "    # kappa_calculation\n",
    "    final_ans = kappa_calculation(one_hot_list1, one_hot_list2)\n",
    "\n",
    "    return final_ans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f24a19a",
   "metadata": {},
   "source": [
    "## Agreement between a pair of reviewers\n",
    "\n",
    "This function calculates the inter-rater agreement between two reviewers for all the parameters of interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3effa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kappa(rater1, rater2):\n",
    "    \"\"\" Function that calculates inter-rater agreement between `rater1` and `rater2` across all parameters\n",
    "    of interest.\n",
    "\n",
    "    Parameters:\n",
    "    rater1, rater2: dataframes corresponding to the two reviewers. It is assumed that the dataframes have a column\n",
    "                    for each parameter of interest (defined within the body of the function), that the numer of rows\n",
    "                    is equal and the answers follow expected formatting (YES/NO, list of tokens, etc.)\n",
    "\n",
    "    Returns:\n",
    "    dict {str: (float, (float, float))}\n",
    "    A collection mapping each parameter to the calculated Cohen's Kappa and confidence interval.\n",
    "    \"\"\"\n",
    "\n",
    "    boolean_columns = ['review','llm', 'structured_data']\n",
    "    non_boolean_columns = ['llm_name', 'list_of_medical_conditions']\n",
    "    result = {}\n",
    "\n",
    "    for column_name in boolean_columns:\n",
    "\n",
    "        try:\n",
    "\n",
    "            List1 = rater1[column_name].to_list()\n",
    "            List2 = rater2[column_name].to_list()\n",
    "\n",
    "            # calculate kappa for this specific boolean column\n",
    "            output = kappa_boolean(List1, List2)\n",
    "\n",
    "            result[column_name] = output\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Error in column', column_name , e)\n",
    "            result[column_name] = 'error'\n",
    "\n",
    "\n",
    "    for column_name in non_boolean_columns:\n",
    "\n",
    "        try:\n",
    "\n",
    "            List1 = rater1[column_name].to_list()\n",
    "            List2 = rater2[column_name].to_list()\n",
    "\n",
    "            output = kappa_non_boolean(List1,List2)\n",
    "\n",
    "            result[column_name] = output\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Error in column', column_name , e)\n",
    "            result[column_name] = 'error'\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8cec4",
   "metadata": {},
   "source": [
    "## Running the code\n",
    "\n",
    "Below we calculate and display the agreement values for each pair of reviewers and consensus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a597c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_reviewer1 = Kappa(GPT,Reviewer1)\n",
    "gpt_resolution = Kappa(GPT,Resolution)\n",
    "reviewer1_resolution = Kappa(Reviewer1,Resolution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936b072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': (0.0, (-1.92, 1.92)),\n",
       " 'llm': (0.47, (0.05, 0.88)),\n",
       " 'structured_data': (0.57, (0.23, 0.9)),\n",
       " 'llm_name': (-0.35, (-0.9, 0.2)),\n",
       " 'list_of_medical_conditions': (-0.24, (-0.34, -0.13))}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_reviewer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05268fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer1_resolution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
