{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "aa179a67",
      "metadata": {
        "id": "aa179a67"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn.metrics\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba9b7e43",
      "metadata": {
        "id": "ba9b7e43"
      },
      "source": [
        "## Data Prep\n",
        "\n",
        "Import the screening results from Reviewer1, Reviewer2 and GPT.\n",
        "\n",
        "Each reviewer evaluated 100 abstracts and answered `YES` or `NO` to two parameters: **AI_ML** and **NLP**. We extract the answers for each parameter and reviewer as python lists and convert `YES`/`NO` to `1` and `0` for easier manipulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "74b36f8a",
      "metadata": {
        "id": "74b36f8a"
      },
      "outputs": [],
      "source": [
        "def prepare_for_kappa(lst):\n",
        "    lst = [value for value in lst if value != \"MISSING\"]\n",
        "    return [1 if value == \"YES\" else 0 for value in lst]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fae10022",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fae10022",
        "outputId": "ec87b896-ff71-4ef2-a376-f76e29e49696"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df_reviewer1 = pd.read_excel('results/Screening_reviewer_1.xlsx')\n",
        "lst_reviewer1_aiml = prepare_for_kappa(df_reviewer1.iloc[:102]['AI_ML'].tolist())\n",
        "lst_reviewer1_nlp = prepare_for_kappa(df_reviewer1.iloc[:102]['NLP'].tolist())\n",
        "# assert that there are 100 answers\n",
        "len(lst_reviewer1_aiml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7138a060",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7138a060",
        "outputId": "eb1299e5-afad-4aad-9935-6cace54a579e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df_reviewer2 = pd.read_excel('results/Screening_reviewer_2.xlsx')\n",
        "lst_reviewer2_aiml = prepare_for_kappa(df_reviewer2.iloc[:102]['AI_ML'].tolist())\n",
        "lst_reviewer2_nlp = prepare_for_kappa(df_reviewer2.iloc[:102]['NLP'].tolist())\n",
        "# assert that there are 100 answers\n",
        "len(lst_reviewer2_aiml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "75dfec74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75dfec74",
        "outputId": "6573900c-bb40-4b77-8464-ccfbf1f76a0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df_reviewer_gpt = pd.read_excel('results/Screening_reviewer_GPT4.xlsx')\n",
        "lst_reviewer_gpt_aiml = prepare_for_kappa(df_reviewer_gpt.iloc[:102]['AI_ML'].tolist())\n",
        "lst_reviewer_gpt_nlp = prepare_for_kappa(df_reviewer_gpt.iloc[:102]['NLP'].tolist())\n",
        "# assert that there are 100 answers\n",
        "len(lst_reviewer_gpt_aiml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3a7df9b2-a710-4326-a5cb-84dc66deb35f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a7df9b2-a710-4326-a5cb-84dc66deb35f",
        "outputId": "03b7ef82-b8d5-43ae-ee1d-199abefe6d9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_resolution = pd.read_excel('results/Screening_resolution.xlsx')\n",
        "lst_resolution_aiml = prepare_for_kappa(df_resolution.iloc[:102]['AI_ML_Resolution'].tolist())\n",
        "lst_resolution_nlp = prepare_for_kappa(df_resolution.iloc[:102]['NLP_Resolution'].tolist())\n",
        "# assert that there are 100 answers\n",
        "len(lst_resolution_aiml)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "356f4493",
      "metadata": {
        "id": "356f4493"
      },
      "source": [
        "## Calculating Inter-rater Agreement\n",
        "\n",
        "We calculate inter-rater agreement as Cohen's Kappa coefficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "Nw8ysb31PKGx",
      "metadata": {
        "id": "Nw8ysb31PKGx"
      },
      "outputs": [],
      "source": [
        "def calculate_kappa_and_CI(list1, list2):\n",
        "  ''' Function that calculates agreement between two graders whose answers are list1 and list2 respectively. Source code:\n",
        "      https://rowannicholls.github.io/python/statistics/agreement/cohens_kappa.html#confidence-intervals\n",
        "\n",
        "      Parameters:\n",
        "      list1, list2: lists of 1s and 0s, where 1 stands for a \"YES\" answer and 0 for a \"NO\" answer\n",
        "\n",
        "      Returns:\n",
        "      a tuple of three numerical values representing the Cohen's Kappa value, lower end of the confidence interval and\n",
        "      upper end of the confidence interval, in this order.\n",
        "  '''\n",
        "\n",
        "  # Create confusion matrix\n",
        "  cm = sklearn.metrics.confusion_matrix(list1, list2)\n",
        "\n",
        "  # Sample size\n",
        "  n = np.sum(cm)\n",
        "\n",
        "  # Expected matrix\n",
        "  sum0 = np.sum(cm, axis=0)\n",
        "  sum1 = np.sum(cm, axis=1)\n",
        "  expected = np.outer(sum0, sum1) / n\n",
        "\n",
        "  # Number of classes\n",
        "  n_classes = cm.shape[0]\n",
        "\n",
        "  # Calculate p_o (the observed proportionate agreement) and\n",
        "  # p_e (the probability of random agreement)\n",
        "  identity = np.identity(n_classes)\n",
        "  p_o = np.sum((identity * cm) / n)\n",
        "  p_e = np.sum((identity * expected) / n)\n",
        "\n",
        "  # Calculate Cohen's kappa\n",
        "  kappa = (p_o - p_e) / (1 - p_e)\n",
        "\n",
        "  # Confidence intervals\n",
        "  se = np.sqrt((p_o * (1 - p_o)) / (n * (1 - p_e)**2))\n",
        "  ci = 1.96 * se * 2\n",
        "  lower = kappa - 1.96 * se\n",
        "  upper = kappa + 1.96 * se\n",
        "\n",
        "  return (kappa, lower, upper)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "11XawH10VJBv",
      "metadata": {
        "id": "11XawH10VJBv"
      },
      "outputs": [],
      "source": [
        "graders = [\"GPT\", \"Reviewer1\", \"Reviewer2\", \"Resolution\"]\n",
        "aiml_lists = [lst_reviewer_gpt_aiml, lst_reviewer1_aiml, lst_reviewer2_aiml, lst_resolution_aiml]\n",
        "nlp_lists = [lst_reviewer_gpt_nlp, lst_reviewer1_nlp, lst_reviewer2_nlp, lst_resolution_nlp]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "MrzuKtOHWd8N",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrzuKtOHWd8N",
        "outputId": "f49953cf-9cae-40d4-db4f-a6fb03b48654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agreement between GPT and Reviewer1 on AI/ML parameter:\n",
            "0.901, (0.806, 0.996)\n",
            "\n",
            "Agreement between GPT and Reviewer2 on AI/ML parameter:\n",
            "0.899, (0.801, 0.996)\n",
            "\n",
            "Agreement between GPT and Resolution on AI/ML parameter:\n",
            "0.925, (0.841, 1.009)\n",
            "\n",
            "Agreement between Reviewer1 and Reviewer2 on AI/ML parameter:\n",
            "0.899, (0.801, 0.996)\n",
            "\n",
            "Agreement between Reviewer1 and Resolution on AI/ML parameter:\n",
            "0.975, (0.926, 1.024)\n",
            "\n",
            "Agreement between Reviewer2 and Resolution on AI/ML parameter:\n",
            "0.923, (0.837, 1.009)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# AI/ML Kappas calculations for each unique pair of graders\n",
        "for grader1_idx in range(0,len(graders)-1):\n",
        "  for grader2_idx in range(grader1_idx+1, len(graders)):\n",
        "    list1 = aiml_lists[grader1_idx]\n",
        "    list2 = aiml_lists[grader2_idx]\n",
        "\n",
        "    kappa, lower, upper = calculate_kappa_and_CI(list1, list2)\n",
        "\n",
        "    print(f\"Agreement between {graders[grader1_idx]} and {graders[grader2_idx]} on AI/ML parameter:\")\n",
        "    print(f\"{kappa:.3f}, ({lower:.3f}, {upper:.3f})\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "-D_SMZP2a4Iy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D_SMZP2a4Iy",
        "outputId": "429c1f5a-5fa9-45f1-d384-136fb44e5e73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agreement between GPT and Reviewer1 on NLP parameter:\n",
            "0.917, (0.824, 1.010)\n",
            "\n",
            "Agreement between GPT and Reviewer2 on NLP parameter:\n",
            "0.971, (0.915, 1.027)\n",
            "\n",
            "Agreement between GPT and Resolution on NLP parameter:\n",
            "0.944, (0.866, 1.021)\n",
            "\n",
            "Agreement between Reviewer1 and Reviewer2 on NLP parameter:\n",
            "0.890, (0.785, 0.996)\n",
            "\n",
            "Agreement between Reviewer1 and Resolution on NLP parameter:\n",
            "0.973, (0.920, 1.026)\n",
            "\n",
            "Agreement between Reviewer2 and Resolution on NLP parameter:\n",
            "0.917, (0.824, 1.010)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# NLP Kappas calculations for each unique pair of graders\n",
        "for grader1_idx in range(0,len(graders)-1):\n",
        "  for grader2_idx in range(grader1_idx+1, len(graders)):\n",
        "    list1 = nlp_lists[grader1_idx]\n",
        "    list2 = nlp_lists[grader2_idx]\n",
        "\n",
        "    kappa, lower, upper = calculate_kappa_and_CI(list1, list2)\n",
        "\n",
        "    print(f\"Agreement between {graders[grader1_idx]} and {graders[grader2_idx]} on NLP parameter:\")\n",
        "    print(f\"{kappa:.3f}, ({lower:.3f}, {upper:.3f})\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating additional metrics\n",
        "\n",
        "Here we calculate accuracy, sensitivity, specificity and F1 score of GPT4 as compared to the resolution answers."
      ],
      "metadata": {
        "id": "A52wAccDhL78"
      },
      "id": "A52wAccDhL78"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_stats(GPT_answers, ground_truth):\n",
        "  \"\"\" Calculates Accuracy, Sensitivity, Specificity and F1 score for a list of binary answers given the ground truth.\n",
        "\n",
        "      Parameters:\n",
        "      GPT_answers, ground_truth: lists of 1s and 0s. Must have the same length.\n",
        "\n",
        "      Returns:\n",
        "      Dictionary mapping each calculated statistic to its corresponding value.\n",
        "  \"\"\"\n",
        "  n = len(GPT_answers)\n",
        "  true_positives = 0\n",
        "  true_negatives = 0\n",
        "  false_positives = 0\n",
        "  false_negatives = 0\n",
        "\n",
        "  for given_answer, correct_answer in zip(GPT_answers, ground_truth):\n",
        "    if given_answer == correct_answer == 1:\n",
        "      true_positives+=1\n",
        "    elif given_answer == correct_answer == 0:\n",
        "      true_negatives+=1\n",
        "    # if reached here we know that given_answer != correct_answer since they can only take values of 1 or 0\n",
        "    elif correct_answer == 1: # then given_answer == 0\n",
        "      false_negatives+=1\n",
        "    else: # correct_answer == 0 and given_answer == 1\n",
        "      false_positives+=1\n",
        "\n",
        "  if not (true_positives+true_negatives+false_positives+false_negatives == n):\n",
        "    raise Exception('Something went wrong, check that your lists are binary and of equal length')\n",
        "\n",
        "  answers_by_category = (true_positives, true_negatives, false_positives, false_negatives)\n",
        "\n",
        "\n",
        "\n",
        "  precision = true_positives/(true_positives+false_positives)   # how many of classified positives are true positives\n",
        "  sensitivity = true_positives/(true_positives+false_negatives) # how many of the actual positives are correctly identified as positives\n",
        "  specificity = true_negatives/(true_negatives+false_positives) # how many of the actual negatives are identified as negatives\n",
        "  accuracy = (true_positives+true_negatives)/n                  # how many of the examples are correctly classified\n",
        "  f1_score = 2*precision*sensitivity/(precision+sensitivity)    # harmonic mean of precision and sensitivity\n",
        "\n",
        "  return {'Results by category (TP, TN, FP, FN)': answers_by_category,\n",
        "          'Accuracy': accuracy,\n",
        "          'Sensitivity': sensitivity,\n",
        "          'Specificity (sensitivity excluded)': specificity,\n",
        "          'F1 score': f1_score}\n"
      ],
      "metadata": {
        "id": "WHwnD0Y6hLgN"
      },
      "id": "WHwnD0Y6hLgN",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We calculate and report the performance metrics of GPT for the concatenated lists for **AI/ML** and **NLP** parameters"
      ],
      "metadata": {
        "id": "PoefRQmDHdPG"
      },
      "id": "PoefRQmDHdPG"
    },
    {
      "cell_type": "code",
      "source": [
        "stats_GPT_consensus_AIML = calculate_stats(lst_reviewer_gpt_aiml, lst_resolution_aiml)\n",
        "stats_GPT_consensus_NLP = calculate_stats(lst_reviewer_gpt_nlp, lst_resolution_nlp)\n",
        "stats_GPT_consensus_overall = calculate_stats([*lst_reviewer_gpt_aiml, *lst_reviewer_gpt_nlp], [*lst_resolution_aiml, *lst_resolution_nlp])\n",
        "\n",
        "print(\"Results based only on AIML parameter: \\n\")\n",
        "for key, val in stats_GPT_consensus_AIML.items():\n",
        "  print(key, \": \", val)\n",
        "\n",
        "print(\"\\n\\nResults based only on NLP parameter: \\n\")\n",
        "for key, val in stats_GPT_consensus_NLP.items():\n",
        "  print(key, \": \", val)\n",
        "\n",
        "print(\"\\n\\nOverall results: \\n\")\n",
        "for key, val in stats_GPT_consensus_overall.items():\n",
        "  print(key, \": \", val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y5ZJ-JwtXPx",
        "outputId": "b3d8b326-fb72-4b2b-f5cd-6191f24b3ca0"
      },
      "id": "_y5ZJ-JwtXPx",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results based only on AIML parameter: \n",
            "\n",
            "Results by category (TP, TN, FP, FN) :  (26, 71, 2, 1)\n",
            "Accuracy :  0.97\n",
            "Sensitivity :  0.9629629629629629\n",
            "Specificity (sensitivity excluded) :  0.9726027397260274\n",
            "F1 score :  0.9454545454545454\n",
            "\n",
            "\n",
            "Results based only on NLP parameter: \n",
            "\n",
            "Results by category (TP, TN, FP, FN) :  (22, 76, 0, 2)\n",
            "Accuracy :  0.98\n",
            "Sensitivity :  0.9166666666666666\n",
            "Specificity (sensitivity excluded) :  1.0\n",
            "F1 score :  0.9565217391304348\n",
            "\n",
            "\n",
            "Overall results: \n",
            "\n",
            "Results by category (TP, TN, FP, FN) :  (48, 147, 2, 3)\n",
            "Accuracy :  0.975\n",
            "Sensitivity :  0.9411764705882353\n",
            "Specificity (sensitivity excluded) :  0.9865771812080537\n",
            "F1 score :  0.9504950495049505\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}