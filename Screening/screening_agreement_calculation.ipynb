{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "aa179a67",
      "metadata": {
        "id": "aa179a67"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba9b7e43",
      "metadata": {},
      "source": [
        "## Data Prep\n",
        "\n",
        "Import the screening results from Reviewer1, Reviewer2 and GPT.\n",
        "\n",
        "Each reviewer evaluated 100 abstracts and answered `YES` or `NO` to two parameters: **AI_ML** and **NLP**. We extract the answers for each parameter and reviewer as python lists and convert `YES`/`NO` to `1` and `0` for easier manipulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "74b36f8a",
      "metadata": {
        "id": "74b36f8a"
      },
      "outputs": [],
      "source": [
        "def prepare_for_kappa(lst):\n",
        "    lst = [value for value in lst if value != \"MISSING\"]\n",
        "    return [1 if value == \"YES\" else 0 for value in lst]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fae10022",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fae10022",
        "outputId": "744259e3-8cc6-4ce0-d816-b9a541137fc8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_reviewer1 = pd.read_excel('Screening_reviewer_1.xlsx')\n",
        "lst_reviewer1_aiml = prepare_for_kappa(df_reviewer1.iloc[:102]['AI_ML'].tolist())\n",
        "lst_reviewer1_nlp = prepare_for_kappa(df_reviewer1.iloc[:102]['NLP'].tolist())\n",
        "# assert that there are 100 answers\n",
        "len(lst_reviewer1_aiml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7138a060",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7138a060",
        "outputId": "114c8052-4a2c-455d-fe62-faebcd8e25de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_reviewer2 = pd.read_excel('Screening_reviewer_2.xlsx')\n",
        "lst_reviewer2_aiml = prepare_for_kappa(df_reviewer2.iloc[:102]['AI_ML'].tolist())\n",
        "lst_reviewer2_nlp = prepare_for_kappa(df_reviewer2.iloc[:102]['NLP'].tolist())\n",
        "# assert that there are 100 answers\n",
        "len(lst_reviewer2_aiml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "75dfec74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75dfec74",
        "outputId": "99d6d8e1-088c-442f-f91b-ef463cc98065"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_reviewer_gpt = pd.read_excel('Screening_reviewer_GPT4.xlsx')\n",
        "lst_reviewer_gpt_aiml = prepare_for_kappa(df_reviewer_gpt.iloc[:102]['AI_ML'].tolist())\n",
        "lst_reviewer_gpt_nlp = prepare_for_kappa(df_reviewer_gpt.iloc[:102]['NLP'].tolist())\n",
        "# assert that there are 100 answers\n",
        "len(lst_reviewer_gpt_aiml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3a7df9b2-a710-4326-a5cb-84dc66deb35f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a7df9b2-a710-4326-a5cb-84dc66deb35f",
        "outputId": "3aa92c5a-f328-448f-a0a5-f3d741264ef8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_resolution = pd.read_excel('Screening_resolution.xlsx')\n",
        "lst_resolution_aiml = prepare_for_kappa(df_resolution.iloc[:102]['AI_ML_Resolution'].tolist())\n",
        "lst_resolution_nlp = prepare_for_kappa(df_resolution.iloc[:102]['NLP_Resolution'].tolist())\n",
        "# assert that there are 100 answers\n",
        "len(lst_resolution_aiml)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "356f4493",
      "metadata": {},
      "source": [
        "## Calculating Inter-rater Agreement\n",
        "\n",
        "We calculate inter-rater agreement as Cohen's Kappa coefficient. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "Nw8ysb31PKGx",
      "metadata": {
        "id": "Nw8ysb31PKGx"
      },
      "outputs": [],
      "source": [
        "def calculate_kappa_and_CI(list1, list2):\n",
        "  ''' Function that calculates agreement between two graders whose answers are list1 and list2 respectively. Source code:\n",
        "      https://rowannicholls.github.io/python/statistics/agreement/cohens_kappa.html#confidence-intervals\n",
        "\n",
        "      Parameters:\n",
        "      list1, list2: lists of 1s and 0s, where 1 stands for a \"YES\" answer and 0 for a \"NO\" answer\n",
        "\n",
        "      Returns:\n",
        "      a tuple of three numerical values representing the Cohen's Kappa value, lower end of the confidence interval and\n",
        "      upper end of the confidence interval, in this order.\n",
        "  '''\n",
        "\n",
        "  # Create confusion matrix\n",
        "  cm = sklearn.metrics.confusion_matrix(list1, list2)\n",
        "\n",
        "  # Sample size\n",
        "  n = np.sum(cm)\n",
        "\n",
        "  # Expected matrix\n",
        "  sum0 = np.sum(cm, axis=0)\n",
        "  sum1 = np.sum(cm, axis=1)\n",
        "  expected = np.outer(sum0, sum1) / n\n",
        "\n",
        "  # Number of classes\n",
        "  n_classes = cm.shape[0]\n",
        "\n",
        "  # Calculate p_o (the observed proportionate agreement) and\n",
        "  # p_e (the probability of random agreement)\n",
        "  identity = np.identity(n_classes)\n",
        "  p_o = np.sum((identity * cm) / n)\n",
        "  p_e = np.sum((identity * expected) / n)\n",
        "\n",
        "  # Calculate Cohen's kappa\n",
        "  kappa = (p_o - p_e) / (1 - p_e)\n",
        "\n",
        "  # Confidence intervals\n",
        "  se = np.sqrt((p_o * (1 - p_o)) / (n * (1 - p_e)**2))\n",
        "  ci = 1.96 * se * 2\n",
        "  lower = kappa - 1.96 * se\n",
        "  upper = kappa + 1.96 * se\n",
        "\n",
        "  return (kappa, lower, upper)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "11XawH10VJBv",
      "metadata": {
        "id": "11XawH10VJBv"
      },
      "outputs": [],
      "source": [
        "graders = [\"GPT\", \"Reviewer1\", \"Reviewer2\", \"Resolution\"]\n",
        "aiml_lists = [lst_reviewer_gpt_aiml, lst_reviewer1_aiml, lst_reviewer2_aiml, lst_resolution_aiml]\n",
        "nlp_lists = [lst_reviewer_gpt_nlp, lst_reviewer1_nlp, lst_reviewer2_nlp, lst_resolution_nlp]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "MrzuKtOHWd8N",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrzuKtOHWd8N",
        "outputId": "54b3a8ee-b97f-43c4-e852-a8bd3847b90b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agreement between GPT and Reviewer1 on AI/ML parameter:\n",
            "0.901\n",
            "0.901, (0.806, 0.996)\n",
            "\n",
            "Agreement between GPT and Reviewer2 on AI/ML parameter:\n",
            "0.899\n",
            "0.899, (0.801, 0.996)\n",
            "\n",
            "Agreement between GPT and Resolution on AI/ML parameter:\n",
            "0.925\n",
            "0.925, (0.841, 1.009)\n",
            "\n",
            "Agreement between Reviewer1 and Reviewer2 on AI/ML parameter:\n",
            "0.899\n",
            "0.899, (0.801, 0.996)\n",
            "\n",
            "Agreement between Reviewer1 and Resolution on AI/ML parameter:\n",
            "0.975\n",
            "0.975, (0.926, 1.024)\n",
            "\n",
            "Agreement between Reviewer2 and Resolution on AI/ML parameter:\n",
            "0.923\n",
            "0.923, (0.837, 1.009)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# AI/ML Kappas calculations for each unique pair of graders\n",
        "for grader1_idx in range(0,len(graders)-1):\n",
        "  for grader2_idx in range(grader1_idx+1, len(graders)):\n",
        "    list1 = aiml_lists[grader1_idx]\n",
        "    list2 = aiml_lists[grader2_idx]\n",
        "\n",
        "    kappa, lower, upper = calculate_kappa_and_CI(list1, list2)\n",
        "\n",
        "    print(f\"Agreement between {graders[grader1_idx]} and {graders[grader2_idx]} on AI/ML parameter:\")\n",
        "    print(f\"{kappa:.3f}, ({lower:.3f}, {upper:.3f})\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "-D_SMZP2a4Iy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D_SMZP2a4Iy",
        "outputId": "b6021276-cdab-41d3-aae2-17e035a367be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agreement between GPT and Reviewer1 on NLP parameter:\n",
            "0.917\n",
            "0.917, (0.824, 1.010)\n",
            "\n",
            "Agreement between GPT and Reviewer2 on NLP parameter:\n",
            "0.971\n",
            "0.971, (0.915, 1.027)\n",
            "\n",
            "Agreement between GPT and Resolution on NLP parameter:\n",
            "0.944\n",
            "0.944, (0.866, 1.021)\n",
            "\n",
            "Agreement between Reviewer1 and Reviewer2 on NLP parameter:\n",
            "0.890\n",
            "0.890, (0.785, 0.996)\n",
            "\n",
            "Agreement between Reviewer1 and Resolution on NLP parameter:\n",
            "0.973\n",
            "0.973, (0.920, 1.026)\n",
            "\n",
            "Agreement between Reviewer2 and Resolution on NLP parameter:\n",
            "0.917\n",
            "0.917, (0.824, 1.010)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# NLP Kappas calculations for each unique pair of graders\n",
        "for grader1_idx in range(0,len(graders)-1):\n",
        "  for grader2_idx in range(grader1_idx+1, len(graders)):\n",
        "    list1 = nlp_lists[grader1_idx]\n",
        "    list2 = nlp_lists[grader2_idx]\n",
        "\n",
        "    kappa, lower, upper = calculate_kappa_and_CI(list1, list2)\n",
        "\n",
        "    print(f\"Agreement between {graders[grader1_idx]} and {graders[grader2_idx]} on NLP parameter:\")\n",
        "    print(f\"{kappa:.3f}, ({lower:.3f}, {upper:.3f})\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
